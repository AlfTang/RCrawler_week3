{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Skill\n",
    "* header with user agent\n",
    "* header with referrer\n",
    "* 如何下載檔案 (例如去如期交所，OTC抓取 *.csv 或 *.zip 檔)\n",
    "* 冷知識： <a href='https://www.youtube.com/watch?v=Ylg4sBl1vPc'>robot.txt</a>\n",
    "* 冷知識：<a href='https://www.youtube.com/watch?v=pFXPnN8Taxw'>拜託你來爬我家！ Crawl Error</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### header 的秘密\n",
    "* user agent\n",
    "    * <a href=' https://www.youtube.com/watch?v=ofFMjmxDfPE'> Prestashop Module Blocking Bad Spiders, Crawlers, Bots by User-Agent - Prestashop Addons</a>\n",
    "* referee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No discussion about crawling pages from the Web can be complete without talking\n",
    "about the Robot Exclusion Protocol. This protocol provides a mechanism for Web\n",
    "server administrators to communicate their file access policies; more specifically to\n",
    "identify files that may not be accessed by a crawler. This is done by keeping a file\n",
    "named robots.txt under the root directory of the Web server (such as http:\n",
    "//www.biz.uiowa.edu/robots.txt). This file provides access policy for\n",
    "different User-agents(robots or crawlers). A User-agent value of “*” denotes a default\n",
    "policy for any crawler that does not match other User-agent values in the file. A\n",
    "number of Disallow entries may be provided for a User-agent. Any URL that starts\n",
    "with the value of a Disallow field must not be retrieved by a crawler matching the\n",
    "User-agent. When a crawler wants to retrieve a page from a Web server, it must\n",
    "first fetch the appropriate robots.txt file and make sure that the URL to be\n",
    "fetched is not disallowed. More details on this exclusion protocol can be found at\n",
    "http://www.robotstxt.org/wc/norobots.html. It is efficient to cache\n",
    "the access policies of a number of servers recently visited by the crawler. This would\n",
    "avoid accessing a robots.txt file each time you need to fetch a URL. However,\n",
    "one must make sure that cache entries remain sufficiently fresh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study I: user agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study II: referee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下載檔案\n",
    "* downloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study III: downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
